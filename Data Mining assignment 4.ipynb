{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AJAY KUMAR ( DATA MINING ASSIGNMENT 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "import timeit\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_curve,f1_score\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.model_selection import KFold \n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q12:QDA on Data-1&2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data):\n",
    "    trn,tst = train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "    return trn,tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalisetion\n",
    "\n",
    "def min_max_norm(data,data_min,data_max):\n",
    "    for column in data.columns[:-1]:\n",
    "        data[column] = (data[column] - data_min[column])/ (data_max[column]-data_min[column])    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data,mean,std):\n",
    "    for column in data.columns[:-1]:\n",
    "        data[column] = (data[column] - mean[column])/ (std[column])    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data classify\n",
    "\n",
    "def classify(data,classes):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    c,l=[],[]\n",
    "    for i in classes:\n",
    "        s=data[data[n-1]==i].drop(columns=(n-1),axis=1)\n",
    "        s_len=len(s)\n",
    "        c.append(s)\n",
    "        l.append(s_len)\n",
    "    return c,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat(data,mu):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    sig=np.zeros((n,n))\n",
    "    for i in range(N):\n",
    "        t=data.iloc[i]-mu\n",
    "        sig=sig+np.outer(t,t)\n",
    "    return sig/(N-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,mu,sig,pi,noc):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    X=data.drop(columns=(n-1),axis=1)\n",
    "    del_x=np.zeros([N,noc])\n",
    "    for i in range(N):\n",
    "        for j in range(noc):\n",
    "            sig_inv=LA.inv(sig[j])\n",
    "            det_sig=LA.det(sig[j])\n",
    "            t1=np.dot(mu[j],np.dot(sig_inv,X.iloc[i]))\n",
    "            t2=-0.5*np.dot(mu[j],np.dot(sig_inv,mu[j]))\n",
    "            t3=np.log(pi[j])\n",
    "            t4=-0.5*np.log(det_sig)\n",
    "            t5=-0.5*np.dot(X.iloc[i],np.dot(sig_inv,X.iloc[i]))\n",
    "            del_x[i][j]=t1+t2+t3+t4+t5\n",
    "    return del_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlda(data,mu,sig,pi,noc):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    X=data.drop(columns=(n-1),axis=1)\n",
    "    for j in range(1,noc):\n",
    "        sig_inv_k=LA.inv(sig[j])\n",
    "        sig_inv_l=LA.inv(sig[j-1])\n",
    "        det_sig=LA.det(sig[j])/LA.det(sig[j-1])\n",
    "        pi_kl=pi[j]/pi[j-1]\n",
    "        t1=np.dot(mu[j],sig_inv_k)-np.dot(mu[j-1],sig_inv_l)\n",
    "        t2=-0.5*(np.dot(mu[j],np.dot(sig_inv_k,mu[j]))-np.dot(mu[j-1],np.dot(sig_inv_l,mu[j-1])))\n",
    "        t3=np.log(pi_kl)\n",
    "        t4=-0.5*np.log(det_sig)\n",
    "        t5=-0.5*(sig_inv_k-sig_inv_l)\n",
    "        a=t5\n",
    "        b=t1\n",
    "        c=t2+t3+t4\n",
    "    return a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance check using confusion matrix\n",
    "\n",
    "def metrics(y_tst,y_p,classes):\n",
    "    d = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "    tp=y_tst.value_counts()[classes[1]]\n",
    "    tn=y_tst.value_counts()[classes[0]]\n",
    "    fp=len(d[(d[0]==classes[1]) & (d[1]==classes[0])])\n",
    "    fn=len(d[(d[0]==classes[0]) & (d[1]==classes[1])])\n",
    "    accuracy =(tp+tn)/(tp+tn+fp+fn)\n",
    "    sensitivity=tp/(tp+fn)\n",
    "    specificity=tn/(tn+fp)\n",
    "    precision=tp/(tp+fp)\n",
    "    fmeasure=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "    return tp,tn,fp,fn,accuracy,sensitivity,specificity,precision,fmeasure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi,mu,sig\n",
      "[0.47368421052631576, 0.5263157894736842]\n",
      "[0    0.481697\n",
      "1    0.302559\n",
      "dtype: float64, 0    0.48309\n",
      "1    0.67930\n",
      "dtype: float64]\n",
      "[array([[ 0.11658502, -0.02211018],\n",
      "       [-0.02211018,  0.05307654]]), array([[0.05708638, 0.02108162],\n",
      "       [0.02108162, 0.04553831]])]\n",
      "\n",
      " No.of Classes is 2\n",
      "\n",
      "The performance metrics are \n",
      "\n",
      "              Number of\n",
      "tp           25.000000\n",
      "tn           32.000000\n",
      "fp            7.000000\n",
      "fn            5.000000\n",
      "accuracy      0.826087\n",
      "sensitivity   0.833333\n",
      "specificity   0.820513\n",
      "precision     0.781250\n",
      "fmeasure      0.806452\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',200)\n",
    "# pd.set_option('display.max_columns',200)\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('data1.csv')#(200*3)\n",
    "\n",
    "# Dropping duplicate rows if any\n",
    "# dup=df[df.duplicated()] #Duplicated rows\n",
    "# print(dup)\n",
    "df.drop_duplicates(keep='first',inplace=True)#(190*3)\n",
    "\n",
    "# Dropping columns having NaN values, if any\n",
    "df.dropna(axis=1,inplace=True)\n",
    "\n",
    "# Dropping columns with all zeros\n",
    "df=df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "#Relabelling the columns \n",
    "df.columns = pd.RangeIndex(len(df.columns))  \n",
    "N,n=df.shape[0],df.shape[1]\n",
    "# print(N,n)\n",
    "\n",
    "# Identifying the various classes and their lengths\n",
    "classes=df[n-1].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "noc=len(classes)\n",
    "# print(classes)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "trn,tst=train_test(df)\n",
    "                                                        \n",
    "# Normalising the data using min_max method\n",
    "data_min,data_max=df.min(),df.max()\n",
    "trn=min_max_norm(trn.copy(),data_min,data_max)\n",
    "tst=min_max_norm(tst.copy(),data_min,data_max)\n",
    "\n",
    "# Using Z-score method\n",
    "# mean=np.mean(df,axis=0)\n",
    "# std=np.std(df,axis=0)\n",
    "# df=z_score(df,mean,std)\n",
    "\n",
    "# Classifying the data into classes\n",
    "cl,l=classify(trn,classes) #Returns two lists for different classes and the number of elements in them\n",
    "# print(cl)\n",
    "\n",
    "# Finding the stats for individual classes\n",
    "pi,mu,sig=[],[],[]\n",
    "for i in range(noc):\n",
    "    p=l[i]/trn.shape[0] \n",
    "    m=cl[i].mean()\n",
    "    s=cov_mat(cl[i],m)\n",
    "    pi.append(p)\n",
    "    mu.append(m)\n",
    "    sig.append(s)\n",
    "print(\"pi,mu,sig\",pi,mu,sig,sep='\\n')\n",
    "\n",
    "# Predicting using the model\n",
    "d=pd.DataFrame(predict(tst,mu,sig,pi,noc))\n",
    "d.columns=classes\n",
    "y_p=d.idxmax(axis=1)\n",
    "# print(y_p)\n",
    "y_tst=tst.iloc[:,n-1]\n",
    "\n",
    "\n",
    "# Final Dataframe for showing actual Vs Prediction\n",
    "s = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "s.columns=['y_tst','y_p']\n",
    "# print(s)\n",
    "\n",
    "# Finding the parameters\n",
    "e,f,g=Qlda(trn,mu,sig,pi,noc)\n",
    "e=e.flatten()\n",
    "# print(e,f,g)\n",
    "\n",
    "x=tst.iloc[:,0:n-1]\n",
    "hyp_plane_1=np.zeros(x.shape[0])\n",
    "hyp_plane_2=np.zeros(x.shape[0])\n",
    "for i in range(x.shape[0]):\n",
    "    x1=x.iloc[i,0]\n",
    "    a=e[3]\n",
    "#     print(a)\n",
    "    b=x1*(e[2]+e[1])+f[1]\n",
    "#     print(b)\n",
    "    c=f[0]*x1 + e[0]*pow(x1,2)+g\n",
    "#     print(c)\n",
    "    d=pow(b,2)-4*a*c\n",
    "    hyp_plane_1[i]=(-b+pow(d,0.5))/(2*a)\n",
    "    hyp_plane_2[i]=(-b-pow(d,0.5))/(2*a)\n",
    "\n",
    "# Results\n",
    "print(\"\\n No.of Classes is\", noc)\n",
    "# print(\"\\n The Quadratic Discriminant Function, del is\\n\", d)\n",
    "\n",
    "# Performance Parameters\n",
    "metrics=pd.DataFrame(metrics(y_tst,y_p,classes))\n",
    "metrics.index=['tp','tn','fp','fn','accuracy','sensitivity','specificity','precision','fmeasure']\n",
    "metrics.columns=['Number of']\n",
    "print(\"\\nThe performance metrics are \\n\\n\",metrics)\n",
    "\n",
    "# Plotting Starts\n",
    "# Hyperplane\n",
    "plt.figure(1,figsize=(12,10))\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.scatter(cl[0][0], cl[0][1], color='r',label='Positive')\n",
    "plt.scatter(cl[1][0], cl[1][1], color='g',label='Negative')\n",
    "plt.scatter(x[0],hyp_plane_1,color='b')\n",
    "plt.scatter(x[0],hyp_plane_2,color='b')\n",
    "plt.title('Hyperplane')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(2,figsize=(12,10))\n",
    "plt.subplot(211)\n",
    "fpr, tpr, threshold = roc_curve(y_tst, y_p)\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(212)\n",
    "cm = confusion_matrix(y_tst, y_p)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "\n",
    "# Probability Density Function for class:-1\n",
    "rv = multivariate_normal.pdf(cl[0],mu[0],sig[0])\n",
    "x,y=np.meshgrid(cl[0][0],cl[0][1])\n",
    "z=np.outer(rv,rv)\n",
    "fig = plt.figure(3)\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "surf=ax.plot_surface(x,y,z,cmap='viridis',cstride=1,rstride=1,antialiased=False,linewidth=0)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Multivariate Gaussian Distribution for Positive class')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "\n",
    "# Probability Density Function for class:1\n",
    "rv = multivariate_normal.pdf(cl[1],mu[1],sig[1])\n",
    "x,y=np.meshgrid(cl[1][0],cl[1][1])\n",
    "z=np.outer(rv,rv)\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(x,y,z,cmap='plasma',cstride=1,rstride=1)\n",
    "ax.set_title('Multivariate Gaussian Distribution for Negative Class')\n",
    "ax.set_xlabel('X1')\n",
    "ax.set_ylabel('X2')\n",
    "\n",
    "# Contour Plot for class:-1\n",
    "fig = plt.figure(4)\n",
    "ax=fig.add_subplot(121)\n",
    "x,y=np.meshgrid(cl[0][0],cl[0][1])\n",
    "rv = multivariate_normal(mu[0],sig[0])\n",
    "pos = np.dstack((x, y))\n",
    "ax.contourf(x, y, rv.pdf(pos))\n",
    "ax.set_title('Filled Contour Plot for class:-1')\n",
    "ax.set_xlabel('feature_x1')\n",
    "ax.set_ylabel('feature_x2')\n",
    "\n",
    "# Contour Plot for class:1\n",
    "ax=fig.add_subplot(122)\n",
    "x,y=np.meshgrid(cl[1][0],cl[1][1])\n",
    "rv = multivariate_normal(mu[1],sig[1])\n",
    "pos = np.dstack((x, y))\n",
    "ax.contourf(x, y, rv.pdf(pos))\n",
    "ax.set_title('Filled Contour Plot for class:1')\n",
    "ax.set_xlabel('feature_x1')\n",
    "ax.set_ylabel('feature_x2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q13:QDA on Data-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(data):\n",
    "    trn,tst = train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "    return trn,tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalisetion\n",
    "\n",
    "def min_max_norm(data,data_min,data_max):\n",
    "    for column in data.columns[:-1]:\n",
    "        data[column] = (data[column] - data_min[column])/ (data_max[column]-data_min[column])    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data,mean,std):\n",
    "    for column in data.columns[:-1]:\n",
    "        data[column] = (data[column] - mean[column])/ (std[column])    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify data\n",
    "\n",
    "def classify(data,classes):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    c,l=[],[]\n",
    "    for i in classes:\n",
    "        s=data[data[n-1]==i].drop(columns=(n-1),axis=1)\n",
    "        s_len=len(s)\n",
    "        c.append(s)\n",
    "        l.append(s_len)\n",
    "    return c,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix\n",
    "\n",
    "def cov_mat(data,mu):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    sig=np.zeros((n,n))\n",
    "    for i in range(N):\n",
    "        t=data.iloc[i]-mu\n",
    "        sig=sig+np.outer(t,t)\n",
    "    return sig/(N-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,mu,sig,pi,noc):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    X=data.drop(columns=(n-1),axis=1)\n",
    "    del_x=np.zeros([N,noc])\n",
    "    for i in range(N):\n",
    "        for j in range(noc):\n",
    "            sig_inv=LA.inv(sig[j])\n",
    "            det_sig=LA.det(sig[j])\n",
    "            t1=np.dot(mu[j],np.dot(sig_inv,X.iloc[i]))\n",
    "            t2=-0.5*np.dot(mu[j],np.dot(sig_inv,mu[j]))\n",
    "            t3=np.log(pi[j])\n",
    "            t4=-0.5*np.log(det_sig)\n",
    "            t5=-0.5*np.dot(X.iloc[i],np.dot(sig_inv,X.iloc[i]))\n",
    "            del_x[i][j]=t1+t2+t3+t4+t5\n",
    "    return del_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlda(data,mu,sig,pi,j):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    X=data.drop(columns=(n-1),axis=1)\n",
    "    sig_inv_k=LA.inv(sig[j])\n",
    "    sig_inv_l=LA.inv(sig[j-1])\n",
    "    det_sig=LA.det(sig[j])/LA.det(sig[j-1])\n",
    "    pi_kl=pi[j]/pi[j-1]\n",
    "    t1=np.dot(mu[j],sig_inv_k)-np.dot(mu[j-1],sig_inv_l)\n",
    "    t2=-0.5*(np.dot(mu[j],np.dot(sig_inv_k,mu[j]))-np.dot(mu[j-1],np.dot(sig_inv_l,mu[j-1])))\n",
    "    t3=np.log(pi_kl)\n",
    "    t4=-0.5*np.log(det_sig)\n",
    "    t5=-0.5*(sig_inv_k-sig_inv_l)\n",
    "    a=t5\n",
    "    b=t1\n",
    "    c=t2+t3+t4\n",
    "    return a,b,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix performance check\n",
    "\n",
    "def metrics(y_tst,y_p,classes):\n",
    "    d = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "    tp=y_tst.value_counts()[classes[1]]\n",
    "    tn=y_tst.value_counts()[classes[0]]\n",
    "    fp=len(d[(d[0]==classes[1]) & (d[1]==classes[0])])\n",
    "    fn=len(d[(d[0]==classes[0]) & (d[1]==classes[1])])\n",
    "    accuracy =(tp+tn)/(tp+tn+fp+fn)\n",
    "    sensitivity=tp/(tp+fn)\n",
    "    specificity=tn/(tn+fp)\n",
    "    precision=tp/(tp+fp)\n",
    "    fmeasure=2*(precision*sensitivity)/(precision+sensitivity)\n",
    "    return tp,tn,fp,fn,accuracy,sensitivity,specificity,precision,fmeasure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi,mu,sig\n",
      "\n",
      "0.3380952380952381\n",
      "0.32857142857142857\n",
      "0.3333333333333333\n",
      "0   -4.079516\n",
      "1   -3.290216\n",
      "2    3.133214\n",
      "dtype: float64\n",
      "0    4.567567\n",
      "1    4.263159\n",
      "2   -2.931937\n",
      "dtype: float64\n",
      "0    0.436242\n",
      "1    0.067545\n",
      "2   -0.136274\n",
      "dtype: float64\n",
      "[[16.18208565 10.70817422  4.4628919 ]\n",
      " [10.70817422 17.83191584  1.55417172]\n",
      " [ 4.4628919   1.55417172  8.50911288]]\n",
      "[[ 7.95764841  0.05135189 -0.7750151 ]\n",
      " [ 0.05135189  4.1741899   0.21473632]\n",
      " [-0.7750151   0.21473632  3.38231092]]\n",
      "[[ 9.35753498 -1.40279642 -0.44463834]\n",
      " [-1.40279642  8.10603858  1.18710371]\n",
      " [-0.44463834  1.18710371  6.30050048]]\n",
      "\n",
      " No.of Classes is 3\n",
      "Accuracy %= 86.66666666666667\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',200)\n",
    "# pd.set_option('display.max_columns',200)\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('data3.csv')#(200*3)\n",
    "\n",
    "# Dropping duplicate rows if any\n",
    "# dup=df[df.duplicated()] #Duplicated rows\n",
    "# print(dup)\n",
    "df.drop_duplicates(keep='first',inplace=True)#(190*3)\n",
    "\n",
    "# Dropping columns having NaN values, if any\n",
    "df.dropna(axis=1,inplace=True)\n",
    "\n",
    "# Dropping columns with all zeros\n",
    "df=df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "#Relabelling the columns \n",
    "df.columns = pd.RangeIndex(len(df.columns))  \n",
    "N,n=df.shape[0],df.shape[1]\n",
    "# print(N,n)\n",
    "\n",
    "# Identifying the various classes and their lengths\n",
    "classes=df[n-1].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "noc=len(classes)\n",
    "# print(classes)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "trn,tst=train_test(df)\n",
    "                                                        \n",
    "# # Normalising the data using min_max method\n",
    "# data_min,data_max=df.min(),df.max()\n",
    "# trn=min_max_norm(trn.copy(),data_min,data_max)\n",
    "# tst=min_max_norm(tst.copy(),data_min,data_max)\n",
    "\n",
    "# Using Z-score method\n",
    "mean=np.mean(df,axis=0)\n",
    "std=np.std(df,axis=0)\n",
    "df=z_score(df,mean,std)\n",
    "\n",
    "# Classifying the data into classes\n",
    "cl,l=classify(trn,classes) #Returns two lists for different classes and the number of elements in them\n",
    "# print(l)\n",
    "\n",
    "# Finding the stats for individual classes\n",
    "pi,mu,sig=[],[],[]\n",
    "for i in range(noc):\n",
    "    p=l[i]/trn.shape[0] \n",
    "    m=cl[i].mean()\n",
    "    s=cov_mat(cl[i],m)\n",
    "    pi.append(p)\n",
    "    mu.append(m)\n",
    "    sig.append(s)\n",
    "print(\"pi,mu,sig\\n\",*pi,*mu,*sig,sep='\\n')\n",
    "\n",
    "# Predicting using the model\n",
    "d=pd.DataFrame(predict(tst,mu,sig,pi,noc))\n",
    "d.columns=classes\n",
    "y_p=d.idxmax(axis=1)\n",
    "# print(y_p)\n",
    "y_tst=tst.iloc[:,n-1]\n",
    "\n",
    "# Final Dataframe for showing actual Vs Prediction\n",
    "s = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "s.columns=['y_tst','y_p']\n",
    "match=len(np.where(s.y_tst==s.y_p)[0])\n",
    "accuracy=match/len(y_tst)\n",
    "\n",
    "# Finding the parameters\n",
    "x=tst.iloc[:,0:n-1]\n",
    "hyp_plane=[]\n",
    "for j in range(1,noc):\n",
    "    hyp_plane_1=np.zeros(x.shape[0])\n",
    "    hyp_plane_2=np.zeros(x.shape[0])\n",
    "    e,f,g=Qlda(trn,mu,sig,pi,j)\n",
    "    e=e.flatten()\n",
    "#     print(e,f,g)\n",
    "    for i in range(x.shape[0]):\n",
    "        x1=x.iloc[i,0]\n",
    "        a=e[3]\n",
    "#         print(a)\n",
    "        b=x1*(e[2]+e[1])+f[1]\n",
    "    #     print(b)\n",
    "        c=f[0]*x1 + e[0]*pow(x1,2)+g\n",
    "    #     print(c)\n",
    "        dis=pow(b,2)-4*a*c\n",
    "        hyp_plane_1[i]=(-b+pow(dis,0.5))/(2*a)\n",
    "        hyp_plane_2[i]=(-b-pow(dis,0.5))/(2*a)\n",
    "    hyp_plane.append(hyp_plane_1)\n",
    "    hyp_plane.append(hyp_plane_2)\n",
    "\n",
    "# Results\n",
    "print(\"\\n No.of Classes is\", noc)\n",
    "print(\"Accuracy %=\",accuracy*100)\n",
    "# print(\"\\nActual Vs Prediction is\\n\",s)\n",
    "# print(\"\\n The Quadratic Discriminant Function, del is\\n\", d)\n",
    "\n",
    "# Plotting Starts\n",
    "# Hyperplane\n",
    "fig = plt.figure(1,figsize=(12,10))\n",
    "plt.ylim(-15, 15)\n",
    "plt.scatter(cl[0][0], cl[0][1], color='r',label='Class-1')\n",
    "plt.scatter(cl[1][0], cl[1][1], color='g',label='Class-2')\n",
    "plt.scatter(cl[2][0], cl[2][1], color='b',label='Class-3')\n",
    "plt.scatter(x[0],hyp_plane[0],color='k')\n",
    "# plt.scatter(x[0],hyp_plane[1],color='k')\n",
    "# plt.scatter(x[0],hyp_plane[2],color='y')\n",
    "plt.scatter(x[0],hyp_plane[3],color='y')\n",
    "plt.title('Hyperplane')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q14-Naive Bayes for Data-4:Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 4\n",
      "Test Values\n",
      "[ 1  1 -1 -1  1 -1  1 -1 -1  1 -1  1 -1  1 -1 -1 -1  1 -1  1  1  1 -1  1\n",
      " -1  1  1 -1  1  1 -1  1  1  1  1 -1 -1 -1 -1  1  1 -1  1  1  1 -1 -1  1\n",
      " -1 -1  1  1]\n",
      "Predicted Values\n",
      "[ 1  1 -1 -1  1 -1  1 -1 -1  1 -1  1 -1  1 -1 -1 -1  1 -1  1  1  1 -1  1\n",
      " -1  1  1 -1  1  1 -1  1 -1  1  1 -1 -1 -1 -1  1  1 -1  1  1  1 -1 -1  1\n",
      " -1 -1  1 -1]\n",
      "Accuracy %= 96.15384615384616\n"
     ]
    }
   ],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "def train_test(data):\n",
    "    trn,tst = train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "    return trn,tst\n",
    "\n",
    "def calculate_prior(df, Y):\n",
    "    classes = sorted(list(df[Y].unique()))\n",
    "    prior = []\n",
    "    for i in classes:\n",
    "        prior.append(len(df[df[Y]==i])/len(df))\n",
    "    return prior\n",
    "\n",
    "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
    "    feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Calculate P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y) * P(Y=y) for all y and find the maximum\n",
    "def naive_bayes_categorical(df, X, Y):\n",
    "    # get feature names\n",
    "    features = list(df.columns)[:-1]\n",
    "\n",
    "    # calculate prior\n",
    "    prior = calculate_prior(df, Y)\n",
    "\n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    for x in X:\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            for i in range(len(features)):\n",
    "                likelihood[j] *= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculate posterior probability (numerator only)\n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "\n",
    "        if np.argmax(post_prob)==0:\n",
    "            Y_pred.append(-1)\n",
    "        else:\n",
    "            Y_pred.append(np.argmax(post_prob))\n",
    "\n",
    "    return np.array(Y_pred)\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('data4.csv')#(171*4)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "print(N,n)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "trn,tst=train_test(df)\n",
    "\n",
    "# Splitting testing data into X and Y\n",
    "X_tst = tst.iloc[:,:-1].values\n",
    "Y_tst = tst.iloc[:,-1].values\n",
    "Y_pred = naive_bayes_categorical(trn, X=X_tst, Y=n-1)\n",
    "\n",
    "# Results\n",
    "print(\"Test Values\",Y_tst,\"Predicted Values\",Y_pred, sep='\\n')\n",
    "s=(Y_tst==Y_pred).sum()\n",
    "accuracy=s/Y_tst.shape[0]\n",
    "print(\"Accuracy %=\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q14-Naive Bayes for Data-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 4\n",
      "Test Values\n",
      "[ 1  1 -1  1  1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1 -1 -1  1 -1  1 -1  1\n",
      " -1  1 -1 -1 -1  1  1  1 -1  1 -1  1  1 -1  1  1 -1  1  1 -1  1 -1  1 -1\n",
      " -1 -1  1 -1  1 -1 -1  1  1 -1  1 -1 -1 -1 -1 -1  1  1]\n",
      "Predicted Values\n",
      "[ 1  1 -1  1 -1 -1  1  1 -1  1  1  1  1 -1  1 -1 -1 -1 -1  1 -1  1 -1  1\n",
      " -1  1 -1 -1 -1  1  1  1 -1  1 -1 -1  1 -1  1  1 -1 -1  1 -1  1 -1  1 -1\n",
      " -1 -1  1 -1  1 -1 -1  1  1  1  1 -1 -1 -1 -1 -1  1  1]\n",
      "Accuracy %= 93.93939393939394\n"
     ]
    }
   ],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "def train_test(data):\n",
    "    trn,tst = train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "    return trn,tst\n",
    "\n",
    "def calculate_prior(df, Y):\n",
    "    classes = sorted(list(df[Y].unique()))\n",
    "    prior = []\n",
    "    for i in classes:\n",
    "        prior.append(len(df[df[Y]==i])/len(df))\n",
    "    return prior\n",
    "\n",
    "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
    "#     feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Calculate P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y) * P(Y=y) for all y and find the maximum\n",
    "def naive_bayes_categorical(df, X, Y):\n",
    "    # get feature names\n",
    "    features = list(df.columns)[:-1]\n",
    "\n",
    "    # calculate prior\n",
    "    prior = calculate_prior(df, Y)\n",
    "\n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    for x in X:\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            for i in range(len(features)):\n",
    "                likelihood[j] *= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculate posterior probability (numerator only)\n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "        if np.argmax(post_prob)==0:\n",
    "            Y_pred.append(-1)\n",
    "        else:\n",
    "            Y_pred.append(1)\n",
    "    return np.array(Y_pred)\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('data5.csv')#(171*4)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "print(N,n)\n",
    "\n",
    "for i in range(n-1):\n",
    "    df[i] = pd.cut(df[i].values, bins = 10, labels = [0,1,2,3,4,5,6,7,8,9])\n",
    "# print(df)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "trn,tst=train_test(df)\n",
    "\n",
    "# Splitting testing data into X and Y\n",
    "X_tst = tst.iloc[:,:-1].values\n",
    "Y_tst = tst.iloc[:,-1].values\n",
    "Y_pred = naive_bayes_categorical(trn, X=X_tst, Y=n-1)\n",
    "\n",
    "# Results\n",
    "print(\"Test Values\",Y_tst,\"Predicted Values\",Y_pred, sep='\\n')\n",
    "s=(Y_tst==Y_pred).sum()\n",
    "accuracy=s/Y_tst.shape[0]\n",
    "print(\"Accuracy %=\",accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q15-Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 4\n",
      "Test Values\n",
      "[1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 0 1\n",
      " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1]\n",
      "Predicted Values\n",
      "[1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1]\n",
      "Accuracy %= 88.88888888888889\n"
     ]
    }
   ],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "def train_test(data):\n",
    "    trn,tst = train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "    return trn,tst\n",
    "\n",
    "def calculate_prior(df, Y):\n",
    "    classes = sorted(list(df[Y].unique()))\n",
    "    prior = []\n",
    "    for i in classes:\n",
    "        prior.append(len(df[df[Y]==i])/len(df))\n",
    "    return prior\n",
    "\n",
    "def calculate_likelihood_categorical(df, feat_name, feat_val, Y, label):\n",
    "#     feat = list(df.columns)\n",
    "    df = df[df[Y]==label]\n",
    "    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n",
    "    return p_x_given_y\n",
    "\n",
    "# Calculate P(X=x1|Y=y)P(X=x2|Y=y)...P(X=xn|Y=y) * P(Y=y) for all y and find the maximum\n",
    "def naive_bayes_categorical(df, X, Y):\n",
    "    # get feature names\n",
    "    features = list(df.columns)[:-1]\n",
    "\n",
    "    # calculate prior\n",
    "    prior = calculate_prior(df, Y)\n",
    "\n",
    "    Y_pred = []\n",
    "    # loop over every data sample\n",
    "    for x in X:\n",
    "        # calculate likelihood\n",
    "        labels = sorted(list(df[Y].unique()))\n",
    "        likelihood = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            for i in range(len(features)):\n",
    "                likelihood[j] *= calculate_likelihood_categorical(df, features[i], x[i], Y, labels[j])\n",
    "\n",
    "        # calculate posterior probability (numerator only)\n",
    "        post_prob = [1]*len(labels)\n",
    "        for j in range(len(labels)):\n",
    "            post_prob[j] = likelihood[j] * prior[j]\n",
    "        if np.argmax(post_prob)==0:\n",
    "            Y_pred.append(0)\n",
    "        else:\n",
    "            Y_pred.append(1)\n",
    "    return np.array(Y_pred)\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('data6.csv')#(171*4)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "print(N,n)\n",
    "\n",
    "for i in range(n-1):\n",
    "    df[i] = pd.cut(df[i].values, bins = 10, labels = [0,1,2,3,4,5,6,7,8,9])\n",
    "# print(df)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "trn,tst=train_test(df)\n",
    "\n",
    "# Splitting testing data into X and Y\n",
    "X_tst = tst.iloc[:,:-1].values\n",
    "Y_tst = tst.iloc[:,-1].values\n",
    "Y_pred = naive_bayes_categorical(trn, X=X_tst, Y=n-1)\n",
    "\n",
    "# Results\n",
    "print(\"Test Values\",Y_tst,\"Predicted Values\",Y_pred, sep='\\n')\n",
    "s=(Y_tst==Y_pred).sum()\n",
    "accuracy=s/Y_tst.shape[0]\n",
    "print(\"Accuracy %=\",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q16-QDA for Ionosphere Data(Binary Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " Result of K-fold method\n",
      "\n",
      " accuracy of each fold :[85.91549295774648, 92.85714285714286, 85.71428571428571, 91.42857142857143, 97.14285714285714]\n",
      "\n",
      " Avg accuracy : 90.61167002012073\n",
      "\n",
      " \n",
      " \n",
      " Result of Holdout method\n",
      "\n",
      " accuracy of each iteration :[87.73584905660377, 84.90566037735849, 90.56603773584905, 95.28301886792453]\n",
      "\n",
      " Avg accuracy : 89.62264150943395\n"
     ]
    }
   ],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test(X,y):\n",
    "    return train_test_split(X,y, test_size = 0.3,shuffle=True)\n",
    "\n",
    "def z_score(data,mean,std):\n",
    "    for column in data.columns:\n",
    "        data[column] = (data[column] - mean[column])/ (std[column])    \n",
    "    return data\n",
    "\n",
    "def classify(X,y,classes):\n",
    "    c,l=[],[]\n",
    "    for i in classes:\n",
    "        s=X[y==i]\n",
    "        s_len=len(s)\n",
    "        c.append(s)\n",
    "        l.append(s_len)\n",
    "    return c,l\n",
    "\n",
    "def cov_mat(data,mu):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    sig=np.zeros((n,n))\n",
    "    for i in range(N):\n",
    "        t=data.iloc[i]-mu\n",
    "        sig=sig+np.outer(t,t)\n",
    "    return sig/N-1\n",
    "\n",
    "def predict(X,mu,sig,pi,noc):\n",
    "    N,n=X.shape[0],X.shape[1]\n",
    "    del_x=np.zeros([N,noc])\n",
    "    for i in range(N):\n",
    "        for j in range(noc):\n",
    "            sig_inv=LA.inv(sig[j])\n",
    "            det_sig=abs(LA.det(sig[j]))\n",
    "            t1=np.dot(mu[j],np.dot(sig_inv,X.iloc[i]))\n",
    "            t2=-0.5*np.dot(mu[j],np.dot(sig_inv,mu[j]))\n",
    "            t3=np.log(pi[j])\n",
    "            t4=-0.5*np.log(det_sig)\n",
    "            t5=-0.5*np.dot(X.iloc[i],np.dot(sig_inv,X.iloc[i]))\n",
    "            del_x[i][j]=t1+t2+t3+t4+t5\n",
    "    return del_x\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('Ionosphere.data')#(351*35)\n",
    "# print(df.head(50))\n",
    "\n",
    "# Dropping columns having NaN values, if any\n",
    "df.dropna(axis=1,inplace=True)\n",
    "\n",
    "# Dropping columns with all zeros and low variance\n",
    "df=df.loc[:, (df != 0).any(axis=0)]#(350*34)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "sel = VarianceThreshold(0.25)\n",
    "X=pd.DataFrame(sel.fit_transform(X))\n",
    "\n",
    "#Relabelling the columns \n",
    "X.columns = pd.RangeIndex(len(X.columns))  \n",
    "N,n=df.shape[0],df.shape[1]\n",
    "\n",
    "# Identifying the various classes and their lengths\n",
    "classes=y.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "noc=len(classes)\n",
    "\n",
    "#K-fold cross validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None) \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_trn , X_tst = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_trn , y_tst = y[train_index] , y[test_index]\n",
    "    \n",
    "    # Using Z-score method\n",
    "    mean=np.mean(X_trn,axis=0)\n",
    "    std=np.std(X_trn,axis=0)\n",
    "    X_trn=z_score(X_trn.copy(),mean,std)\n",
    "    X_tst=z_score(X_tst.copy(),mean,std)\n",
    "     \n",
    "    # Classifying the data into classes\n",
    "    cl,l=classify(X_trn,y_trn,classes) #Returns two lists for different classes and the number of elements in them\n",
    "\n",
    "    # Finding the stats for individual classes\n",
    "    pi,mu,sig=[],[],[]\n",
    "    for i in range(noc):\n",
    "        p=l[i]/X_trn.shape[0] \n",
    "        m=cl[i].mean()\n",
    "        s=cov_mat(cl[i],m)\n",
    "        pi.append(p)\n",
    "        mu.append(m)\n",
    "        sig.append(s)\n",
    "\n",
    "    # Predicting using the model\n",
    "    d=pd.DataFrame(predict(X_tst,mu,sig,pi,noc))\n",
    "    d.columns=classes\n",
    "    y_p=d.idxmax(axis=1)\n",
    "\n",
    "    # Final Dataframe for showing actual Vs Prediction\n",
    "    s = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "    s.columns=['y_tst','y_p']\n",
    "    match=len(np.where(s.y_tst==s.y_p)[0])\n",
    "    accuracy=match*100/len(y_tst)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "avg_acc_score = sum(acc_score)/k\n",
    "print('\\n \\n \\n Result of K-fold method') \n",
    "print('\\n accuracy of each fold :{}'.format(acc_score))\n",
    "print('\\n Avg accuracy : {}'.format(avg_acc_score))\n",
    "\n",
    "# Holdout Method\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "acc_score=[]\n",
    "threshold=[0,0.1,0.2,.25]\n",
    "\n",
    "for i in threshold:\n",
    "    sel = VarianceThreshold(i)\n",
    "    X=pd.DataFrame(sel.fit_transform(X))\n",
    "    \n",
    "    # Splitting into training and testing data\n",
    "    X_trn,X_tst,y_trn,y_tst=train_test(X,y)\n",
    "\n",
    "    # Using Z-score method\n",
    "    mean=np.mean(X_trn,axis=0)\n",
    "    std=np.std(X_trn,axis=0)\n",
    "    X_trn=z_score(X_trn.copy(),mean,std)\n",
    "    X_tst=z_score(X_tst.copy(),mean,std)\n",
    "\n",
    "    #Relabelling the columns \n",
    "    X_trn.columns = pd.RangeIndex(len(X_trn.columns)) \n",
    "    X_tst.columns = pd.RangeIndex(len(X_tst.columns))\n",
    "    \n",
    "    # Classifying the data into classes\n",
    "    cl,l=classify(X_trn,y_trn,classes) #Returns two lists for different classes and the number of elements in them\n",
    "\n",
    "    # Finding the stats for individual classes\n",
    "    pi,mu,sig=[],[],[]\n",
    "    for i in range(noc):\n",
    "        p=l[i]/X_trn.shape[0] \n",
    "        m=cl[i].mean()\n",
    "        s=cov_mat(cl[i],m)\n",
    "        pi.append(p)\n",
    "        mu.append(m)\n",
    "        sig.append(s)\n",
    "\n",
    "    # Predicting using the model\n",
    "    d=pd.DataFrame(predict(X_tst,mu,sig,pi,noc))\n",
    "    d.columns=classes\n",
    "    y_p=d.idxmax(axis=1)\n",
    "\n",
    "    # Final Dataframe for showing actual Vs Prediction\n",
    "    s = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "    s.columns=['y_tst','y_p']\n",
    "    match=len(np.where(s.y_tst==s.y_p)[0])\n",
    "    accuracy=match*100/len(y_tst)\n",
    "    acc_score.append(accuracy)\n",
    "\n",
    "avg_acc_score = sum(acc_score)/len(threshold)\n",
    "print('\\n \\n \\n Result of Holdout method') \n",
    "print('\\n accuracy of each iteration :{}'.format(acc_score))\n",
    "print('\\n Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q17-QDA for Abalone Data(Multiclass Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[1, 1, 11, 49, 90, 189, 294, 417, 520, 476, 371, 195, 145, 96, 79, 52, 43, 32, 22, 23, 9, 4, 7, 2, 1, 1, 1, 1]\n",
      "[0.00031928480204342275, 0.00031928480204342275, 0.00351213282247765, 0.015644955300127713, 0.028735632183908046, 0.0603448275862069, 0.09386973180076628, 0.1331417624521073, 0.16602809706257982, 0.15197956577266922, 0.11845466155810984, 0.06226053639846743, 0.046296296296296294, 0.03065134099616858, 0.025223499361430396, 0.016602809706257982, 0.013729246487867178, 0.010217113665389528, 0.0070242656449553, 0.007343550446998723, 0.0028735632183908046, 0.001277139208173691, 0.002234993614303959, 0.0006385696040868455, 0.00031928480204342275, 0.00031928480204342275, 0.00031928480204342275, 0.00031928480204342275]\n",
      "\n",
      " No.of Classes is 28\n",
      "Accuracy %= 22.48803827751196\n"
     ]
    }
   ],
   "source": [
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "def train_test(X,y):\n",
    "    return train_test_split(X,y, test_size = 0.3,shuffle=True)\n",
    "\n",
    "def z_score(data,mean,std):\n",
    "    for column in data.columns:\n",
    "        data[column] = (data[column] - mean[column])/ (std[column])    \n",
    "    return data\n",
    "\n",
    "def classify(X,y,classes):\n",
    "    c,l=[],[]\n",
    "    for i in classes:\n",
    "        s=X[y==i]\n",
    "        s_len=len(s)\n",
    "        c.append(s)\n",
    "        l.append(s_len)\n",
    "    return c,l\n",
    "\n",
    "def cov_mat(data,mu):\n",
    "    N,n=data.shape[0],data.shape[1]\n",
    "    sig=np.identity(n)\n",
    "    for i in range(N):\n",
    "        t=data.iloc[i]-mu\n",
    "        sig=sig+np.outer(t,t)\n",
    "    return sig/N-1\n",
    "\n",
    "def predict(X,mu,sig,pi,noc):\n",
    "    N,n=X.shape[0],X.shape[1]\n",
    "    del_x=np.zeros([N,noc])\n",
    "    for i in range(N):\n",
    "        for j in range(noc):\n",
    "            if(pi[j]*100000>1000):\n",
    "                sig_inv=LA.inv(sig[j])\n",
    "                det_sig=abs(LA.det(sig[j]))\n",
    "                t1=np.dot(mu[j],np.dot(sig_inv,X.iloc[i]))\n",
    "                t2=-0.5*np.dot(mu[j],np.dot(sig_inv,mu[j]))\n",
    "                t3=np.log(pi[j])\n",
    "                t4=-0.5*np.log(det_sig)\n",
    "                t5=-0.5*np.dot(X.iloc[i],np.dot(sig_inv,X.iloc[i]))\n",
    "                del_x[i][j]=t1+t2+t3+t4+t5\n",
    "    return del_x\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('abalone.data')\n",
    "# print(df.head(50))\n",
    "\n",
    "# Dropping columns having NaN values, if any\n",
    "df.dropna(axis=1,inplace=True)\n",
    "\n",
    "# Dropping columns with all zeros and low variance\n",
    "df=df.loc[:, (df != 0).any(axis=0)]\n",
    "df[0].replace(to_replace=['M','F','I'],value=[-1,0,1],inplace=True)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "# sel = VarianceThreshold(0.1)\n",
    "# X=pd.DataFrame(sel.fit_transform(X))\n",
    "\n",
    "#Relabelling the columns \n",
    "X.columns = pd.RangeIndex(len(X.columns))  \n",
    "N,n=df.shape[0],df.shape[1]\n",
    "\n",
    "# Identifying the various classes and their lengths\n",
    "classes=y.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "noc=len(classes)\n",
    "print(noc)\n",
    "\n",
    "# Splitting into training and testing groups\n",
    "X_trn,X_tst,y_trn,y_tst=train_test_split(X,y)\n",
    "    \n",
    "# Using Z-score method\n",
    "mean=np.mean(X_trn,axis=0)\n",
    "std=np.std(X_trn,axis=0)\n",
    "X_trn=z_score(X_trn.copy(),mean,std)\n",
    "X_tst=z_score(X_tst.copy(),mean,std)\n",
    "\n",
    "# Classifying the data into classes\n",
    "cl,l=classify(X_trn,y_trn,classes) #Returns two lists for different classes and the number of elements in them\n",
    "print(l)\n",
    "\n",
    "# Finding the stats for individual classes\n",
    "pi,mu,sig=[],[],[]\n",
    "for i in range(noc):\n",
    "    p=l[i]/X_trn.shape[0] \n",
    "    m=cl[i].mean()\n",
    "    s=cov_mat(cl[i],m)\n",
    "    pi.append(p)\n",
    "    mu.append(m)\n",
    "    sig.append(s)\n",
    "print(pi)\n",
    "\n",
    "# Predicting using the model\n",
    "d=pd.DataFrame(predict(X_tst,mu,sig,pi,noc))\n",
    "d.columns=classes\n",
    "y_p=d.idxmax(axis=1)\n",
    "\n",
    "# Final Dataframe for showing actual Vs Prediction\n",
    "s = pd.concat([y_tst.reset_index(drop=True), y_p.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "s.columns=['y_tst','y_p']\n",
    "match=len(np.where(s.y_tst==s.y_p)[0])\n",
    "accuracy=match/len(y_tst)\n",
    "\n",
    "# Results\n",
    "print(\"\\n No.of Classes is\", noc)\n",
    "print(\"Accuracy %=\",accuracy*100)\n",
    "# print(\"\\nActual Vs Prediction is\\n\",s)\n",
    "# print(\"\\n The Quadratic Discriminant Function, del is\\n\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q17-Multinomial Naive Bayes : Abalone Data(Multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: 0.1340996168582374, 6: 0.06034482758620679, 11: 0.10855683269476334, 20: 0.0063856960408684525, 19: 0.00606641123882503, 10: 0.1542145593869738, 13: 0.051404853128991006, 12: 0.06864623243933574, 17: 0.015006385696040861, 23: 0.002554278416347382, 7: 0.08971902937420151, 9: 0.16762452107279804, 26: 0.00031928480204342275, 4: 0.012771392081736903, 15: 0.022988505747126464, 14: 0.030332056194125222, 5: 0.030012771392081798, 18: 0.01117496807151979, 16: 0.016283524904214555, 3: 0.004150702426564495, 21: 0.003831417624521072, 29: 0.00031928480204342275, 24: 0.0006385696040868455, 22: 0.0015964240102171138, 2: 0.00031928480204342275, 1: 0.00031928480204342275, 27: 0.00031928480204342275}\n",
      "       0          1          2          3          4          5          6   \\\n",
      "0     0.0 -54.184969 -50.661394 -34.287339 -30.908763 -24.643178 -24.124265   \n",
      "1     0.0 -54.126550 -50.603069 -35.856824 -32.827016 -25.004315 -25.461600   \n",
      "2     0.0 -41.581927 -38.531603 -24.817541 -21.928496 -19.113532 -18.850475   \n",
      "3     0.0 -28.551841 -26.814646 -13.060937 -10.582873 -10.813159 -14.261535   \n",
      "4     0.0 -44.226959 -40.988314 -27.322213 -24.047645 -19.348422 -18.098089   \n",
      "...   ...        ...        ...        ...        ...        ...        ...   \n",
      "1040  0.0 -41.228760 -38.763797 -25.834427 -23.105480 -18.362532 -19.536641   \n",
      "1041  0.0 -42.609708 -39.358082 -25.531659 -22.214118 -18.244097 -17.012644   \n",
      "1042  0.0 -41.192686 -38.981622 -24.188398 -21.567861 -19.197518 -21.557896   \n",
      "1043  0.0 -38.539545 -36.042689 -22.722590 -19.890023 -16.923100 -17.932554   \n",
      "1044  0.0 -45.670971 -42.447148 -28.963081 -25.736957 -19.939797 -19.014631   \n",
      "\n",
      "             7          8          9   ...         20         21         22  \\\n",
      "0    -27.924555 -32.035216 -35.368969  ... -45.432798 -45.300625 -46.336053   \n",
      "1    -29.109827 -33.073765 -36.274208  ... -46.313057 -46.203806 -47.180006   \n",
      "2    -21.890232 -25.287636 -28.240051  ... -37.057377 -36.980796 -38.168602   \n",
      "3    -18.183174 -23.108586 -27.669158  ... -38.749783 -38.436724 -41.023191   \n",
      "4    -20.232834 -22.161346 -23.542026  ... -29.921309 -30.089903 -30.322726   \n",
      "...         ...        ...        ...  ...        ...        ...        ...   \n",
      "1040 -22.489024 -25.820611 -28.661620  ... -37.721344 -37.689583 -38.809594   \n",
      "1041 -19.070413 -20.971258 -22.330827  ... -28.705105 -28.903765 -29.177111   \n",
      "1042 -25.984842 -31.400657 -36.391815  ... -48.563214 -48.150093 -50.407026   \n",
      "1043 -20.860092 -24.116018 -26.971579  ... -35.606447 -35.534801 -36.684937   \n",
      "1044 -21.203077 -23.183946 -24.570479  ... -31.186942 -31.360594 -31.552882   \n",
      "\n",
      "             23         24   25         26         27   28         29  \n",
      "0    -43.739168 -51.290765  0.0 -52.501628 -49.923955  0.0 -49.019808  \n",
      "1    -44.665039 -52.092492  0.0 -53.366515 -50.864060  0.0 -49.402814  \n",
      "2    -35.411203 -42.130454  0.0 -43.633975 -40.070956  0.0 -40.247132  \n",
      "3    -36.102442 -44.089999  0.0 -49.507815 -37.822216  0.0 -39.623462  \n",
      "4    -29.297382 -34.355480  0.0 -33.076280 -35.774480  0.0 -34.583195  \n",
      "...         ...        ...  ...        ...        ...  ...        ...  \n",
      "1040 -36.242081 -42.777423  0.0 -44.217668 -40.982837  0.0 -40.382819  \n",
      "1041 -28.083203 -33.088218  0.0 -31.772691 -34.509755  0.0 -33.407993  \n",
      "1042 -45.499920 -54.588224  0.0 -59.386253 -48.431683  0.0 -49.609684  \n",
      "1043 -34.010861 -40.481330  0.0 -41.903715 -38.290941  0.0 -38.406135  \n",
      "1044 -30.703439 -35.706633  0.0 -34.375702 -37.240270  0.0 -35.745115  \n",
      "\n",
      "[1045 rows x 30 columns]\n",
      "\n",
      " No.of Classes is 28\n",
      "Accuracy %= 0.0\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.prior = None\n",
    "        # Alpha is a value, usually 1, in order to avoid the situation of features with 0 occurances\n",
    "        # totally canceling out the numerator.\n",
    "        self.alpha = alpha\n",
    "        self.col_probs = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        feature_probs = {}\n",
    "        prior_probs = {}\n",
    "        lb=[]\n",
    "        for row, label in zip(X, y):\n",
    "            if label not in feature_probs:\n",
    "                prior_probs[label] = 0\n",
    "                feature_probs[label] = [self.alpha] * len(X[0])\n",
    "                lb.append(label)\n",
    "            prior_probs[label] += (1 / len(y))\n",
    "            \n",
    "            for feature_idx, feature in enumerate(row):\n",
    "                feature_probs[label][feature_idx] += feature\n",
    "        \n",
    "        self.prior = prior_probs\n",
    "        self.lb=lb\n",
    "\n",
    "        for label, counts in feature_probs.items():\n",
    "            for feature_idx, feature in enumerate(counts):\n",
    "                feature_probs[label][feature_idx] = feature / sum(counts)\n",
    "        self.feature_probs = feature_probs\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        N,n=X.shape[0],X.shape[1]\n",
    "        \n",
    "        del_x=np.zeros([N,30])\n",
    "        for i in range(N):\n",
    "            for j in self.prior:\n",
    "                t1=np.log(self.prior[j])\n",
    "                t2=np.dot(X[i],np.log(self.feature_probs[j]))\n",
    "                del_x[i][j]=t1+t2\n",
    "        return del_x\n",
    "\n",
    "def read_data(data):\n",
    "    cwd=os.getcwd()\n",
    "    path=os.path.join(cwd,'assign_4', data)\n",
    "    df=pd.read_csv(path,delimiter=',',header=None,na_values='?')\n",
    "    return df\n",
    "\n",
    "def train_test(data):\n",
    "    return train_test_split(data, test_size = 0.3,shuffle=True)\n",
    "\n",
    "\n",
    "# Reading the data\n",
    "df=read_data('abalone.data')\n",
    "\n",
    "# Dropping columns having NaN values, if any\n",
    "df.dropna(axis=1,inplace=True)\n",
    "\n",
    "# Dropping columns with all zeros and low variance\n",
    "df=df.loc[:, (df != 0).any(axis=0)]\n",
    "df[0].replace(to_replace=['M','F','I'],value=[-1,0,1],inplace=True)\n",
    "N,n=df.shape[0],df.shape[1]\n",
    "\n",
    "for i in range(n-1):\n",
    "    df[i] = pd.cut(df[i].values, bins = 10, labels = [0,1,2,3,4,5,6,7,8,9])\n",
    "    \n",
    "X=df.iloc[:,:-1].values\n",
    "y=df.iloc[:,-1].values\n",
    "\n",
    "# Splitting testing data into X and Y\n",
    "X_trn,X_tst,y_trn,y_tst=train_test_split(X,y)\n",
    "\n",
    "# Identifying the various classes and their lengths\n",
    "classes=df.iloc[:,-1].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "noc=len(classes)\n",
    "\n",
    "clf = NaiveBayes()\n",
    "clf.fit(X_trn, y_trn)\n",
    "print(clf.prior)\n",
    "\n",
    "# Predicting using the model\n",
    "d=pd.DataFrame(clf.predict(X_tst))\n",
    "c=d.loc[(d!=0).any(axis=1)]\n",
    "print(c)\n",
    "y_p=d.idxmax(axis=1)\n",
    "\n",
    "# Final Dataframe for showing actual Vs Prediction\n",
    "s = pd.concat([pd.DataFrame(y_tst), y_p.reset_index(drop=True),d.reset_index(drop=True)],axis=1,ignore_index=True)\n",
    "match=len(np.where(s[0]==s[1])[0])\n",
    "accuracy=match/len(y_tst)\n",
    "\n",
    "# Results\n",
    "print(\"\\n No.of Classes is\",noc )\n",
    "print(\"Accuracy %=\",accuracy*100)\n",
    "# print(\"\\nActual Vs Prediction is\\n\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
